{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, torchtext\n",
    "from torchtext.functional import to_tensor\n",
    "from torchtext.transforms import SentencePieceTokenizer\n",
    "xlmr_base = torchtext.models.XLMR_BASE_ENCODER\n",
    "model = xlmr_base.get_model()\n",
    "transform = xlmr_base.transform()\n",
    "input_batch = [\"Hello world\", \"How are you!\"]\n",
    "model_input = to_tensor(transform(input_batch), padding_value=1)\n",
    "output = model(model_input)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models import TextClassificationModel, zip_ssl\n",
    "from datasets import AGNEWS\n",
    "from utils import train, eval, pearson, acc, nomean, pearson_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 2048\n",
    "EMBED_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = AGNEWS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "num_classes = 1\n",
    "LR=1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/josegfer/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "# from utils import train\n",
    "\n",
    "# Initialize RoBERTa tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "roberta_model.to(device)\n",
    "\n",
    "# Define the optimizer for the whole model (both RoBERTa and the classification head)\n",
    "optimizer = optim.AdamW(list(roberta_model.parameters()), lr=LR)\n",
    "\n",
    "# Function to get RoBERTa embeddings for a list of sentences\n",
    "def model(sentences):\n",
    "    # Tokenize the sentences\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to the device\n",
    "\n",
    "    # Get the embeddings (without using torch.no_grad())\n",
    "    outputs = roberta_model(**inputs, output_hidden_states=True)\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    embeddings = outputs.hidden_states[-1]  # Last layer embeddings\n",
    "\n",
    "    # Average the embeddings along the max_sequence_length dimension\n",
    "    embeddings_avg = embeddings.mean(dim=1)\n",
    "\n",
    "    return embeddings_avg\n",
    "\n",
    "# Assuming you have defined the AGNEWS class and its methods correctly\n",
    "ds = AGNEWS()\n",
    "BATCH_SIZE = 16\n",
    "train_loader, val_loader, test_loader = ds.loader(BATCH_SIZE, ssl=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearson(delta, ncd):\n",
    "    vx = delta - torch.mean(delta)\n",
    "    vy = ncd - torch.mean(ncd)\n",
    "\n",
    "    return F.cosine_similarity(vx, vy, dim = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "from tqdm import tqdm\n",
    "def train(model, loader, criterion, optimizer, ssl = False):\n",
    "    cost = 0\n",
    "    corr = 0\n",
    "    # for idx, (label, text, offsets, classes, ncd) in tqdm(enumerate(loader)):\n",
    "    idx_batch = np.arange(loader.batch_size)\n",
    "    for idx, (label, text, offsets, classes, ncd) in (enumerate(loader)):\n",
    "        if ssl:\n",
    "            # np.random.shuffle(idx_batch)\n",
    "            # btsz = len(offsets) // 2\n",
    "            h = model(label)\n",
    "            delta = F.pairwise_distance(h[:btsz], h[btsz:]) / 10\n",
    "            # delta = F.pairwise_distance(h[idx_batch[:btsz]], h[idx_batch[btsz:]]) / 10\n",
    "            # print(idx, delta.mean())\n",
    "            loss = criterion(delta, ncd)\n",
    "        else:\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "\n",
    "        cost += loss.item()\n",
    "        corr += pearson(delta, ncd).cpu()\n",
    "        print(delta.mean(), loss.item(), pearson(delta, ncd).cpu())\n",
    "    return cost / loader.dataset.__len__(), corr / loader.dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maiorpossivel = 0\n",
    "# for idx, (label, text, offsets, classes, ncd) in tqdm(enumerate(train_loader)):\n",
    "#     btsz = len(offsets) // 2\n",
    "#     h = model(label)\n",
    "#     delta = F.pairwise_distance(h[:btsz], h[btsz:])\n",
    "#     if maiorpossivel < delta.max().cpu():\n",
    "#         maiorpossivel = delta.max().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3402, device='cuda:0', grad_fn=<MeanBackward0>) 0.0003534742572810501 tensor(0.8724, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3202, device='cuda:0', grad_fn=<MeanBackward0>) 0.0005003104452043772 tensor(0.8751, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3263, device='cuda:0', grad_fn=<MeanBackward0>) 0.0002805616823025048 tensor(0.9579, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3068, device='cuda:0', grad_fn=<MeanBackward0>) 0.00202717212960124 tensor(0.8139, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3167, device='cuda:0', grad_fn=<MeanBackward0>) 0.0002652944822330028 tensor(0.9770, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3144, device='cuda:0', grad_fn=<MeanBackward0>) 0.005711662117391825 tensor(0.8773, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.2996, device='cuda:0', grad_fn=<MeanBackward0>) 0.0002805099938996136 tensor(0.9108, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3239, device='cuda:0', grad_fn=<MeanBackward0>) 0.0008652654942125082 tensor(0.7735, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.2942, device='cuda:0', grad_fn=<MeanBackward0>) 0.00017120910342782736 tensor(0.9230, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3101, device='cuda:0', grad_fn=<MeanBackward0>) 0.000972026726230979 tensor(0.8704, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3072, device='cuda:0', grad_fn=<MeanBackward0>) 0.0008654986741021276 tensor(0.7486, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3394, device='cuda:0', grad_fn=<MeanBackward0>) 0.000847871764563024 tensor(0.9223, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3154, device='cuda:0', grad_fn=<MeanBackward0>) 0.000514214567374438 tensor(0.9742, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3477, device='cuda:0', grad_fn=<MeanBackward0>) 0.0008030859753489494 tensor(0.6533, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>) 0.0005151072982698679 tensor(0.9568, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3221, device='cuda:0', grad_fn=<MeanBackward0>) 0.0008642097818665206 tensor(0.9047, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3454, device='cuda:0', grad_fn=<MeanBackward0>) 0.0019349243957549334 tensor(0.9447, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3138, device='cuda:0', grad_fn=<MeanBackward0>) 0.0013148256111890078 tensor(0.8942, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3137, device='cuda:0', grad_fn=<MeanBackward0>) 0.0013781138695776463 tensor(0.8478, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3216, device='cuda:0', grad_fn=<MeanBackward0>) 0.0002876656362786889 tensor(0.8573, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3048, device='cuda:0', grad_fn=<MeanBackward0>) 0.00037106676609255373 tensor(0.9084, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3062, device='cuda:0', grad_fn=<MeanBackward0>) 0.0003800041740760207 tensor(0.7237, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3156, device='cuda:0', grad_fn=<MeanBackward0>) 0.0002609195071272552 tensor(0.9676, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3325, device='cuda:0', grad_fn=<MeanBackward0>) 0.0007352916873060167 tensor(0.6016, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3005, device='cuda:0', grad_fn=<MeanBackward0>) 0.0002967699256259948 tensor(0.9629, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3107, device='cuda:0', grad_fn=<MeanBackward0>) 0.0006809996557421982 tensor(0.8662, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3049, device='cuda:0', grad_fn=<MeanBackward0>) 0.0007988957222551107 tensor(0.9372, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3010, device='cuda:0', grad_fn=<MeanBackward0>) 0.0010224285069853067 tensor(0.2311, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3353, device='cuda:0', grad_fn=<MeanBackward0>) 0.001778686884790659 tensor(0.7021, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3185, device='cuda:0', grad_fn=<MeanBackward0>) 0.00040235405322164297 tensor(0.9431, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3266, device='cuda:0', grad_fn=<MeanBackward0>) 0.0004722768790088594 tensor(0.8442, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3709, device='cuda:0', grad_fn=<MeanBackward0>) 0.004004797898232937 tensor(0.8905, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3228, device='cuda:0', grad_fn=<MeanBackward0>) 0.0007522455998696387 tensor(0.9648, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3237, device='cuda:0', grad_fn=<MeanBackward0>) 0.0004510617000050843 tensor(0.9700, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3187, device='cuda:0', grad_fn=<MeanBackward0>) 0.0006100800819694996 tensor(0.9620, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3161, device='cuda:0', grad_fn=<MeanBackward0>) 0.0005374819738790393 tensor(0.8836, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3089, device='cuda:0', grad_fn=<MeanBackward0>) 0.0005716555751860142 tensor(0.5320, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3290, device='cuda:0', grad_fn=<MeanBackward0>) 0.00041367532685399055 tensor(0.8045, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3359, device='cuda:0', grad_fn=<MeanBackward0>) 0.00026351772248744965 tensor(0.8354, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.2852, device='cuda:0', grad_fn=<MeanBackward0>) 0.00039907408063299954 tensor(0.9505, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3032, device='cuda:0', grad_fn=<MeanBackward0>) 0.0005564815946854651 tensor(0.9362, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3245, device='cuda:0', grad_fn=<MeanBackward0>) 0.0005148201598785818 tensor(0.9729, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3099, device='cuda:0', grad_fn=<MeanBackward0>) 0.0006847693002782762 tensor(0.7775, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.2922, device='cuda:0', grad_fn=<MeanBackward0>) 0.00029067875584587455 tensor(0.9248, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3073, device='cuda:0', grad_fn=<MeanBackward0>) 0.0004969537258148193 tensor(0.9678, grad_fn=<ToCopyBackward0>)\n",
      "tensor(0.3015, device='cuda:0', grad_fn=<MeanBackward0>) 0.0003332268388476223 tensor(0.9159, grad_fn=<ToCopyBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss_train, corr_train \u001b[39m=\u001b[39m train(model, train_loader, criterion, optimizer, ssl \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[57], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer, ssl)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39m# torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 26\u001b[0m cost \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     27\u001b[0m corr \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m pearson(delta, ncd)\u001b[39m.\u001b[39mcpu()\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(delta\u001b[39m.\u001b[39mmean(), loss\u001b[39m.\u001b[39mitem(), pearson(delta, ncd)\u001b[39m.\u001b[39mcpu())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_train, corr_train = train(model, train_loader, criterion, optimizer, ssl = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train, corr_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "152it [00:05, 27.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m idx, (label, text, offsets, classes, ncd) \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(test_loader)):\n\u001b[1;32m      6\u001b[0m     btsz \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(offsets) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m----> 7\u001b[0m     h \u001b[39m=\u001b[39m model(label)\n\u001b[1;32m      8\u001b[0m     delta \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mpairwise_distance(h[:btsz], h[btsz:]) \u001b[39m/\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m      9\u001b[0m     D \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((D, delta\u001b[39m.\u001b[39mcpu()))\n",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m, in \u001b[0;36mmodel\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     19\u001b[0m inputs \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems()}  \u001b[39m# Move inputs to the device\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[39m# Get the embeddings (without using torch.no_grad())\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m outputs \u001b[39m=\u001b[39m roberta_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, output_hidden_states\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     23\u001b[0m \u001b[39mdel\u001b[39;00m inputs\n\u001b[1;32m     24\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:844\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    835\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    837\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    838\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    839\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    842\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    843\u001b[0m )\n\u001b[0;32m--> 844\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    845\u001b[0m     embedding_output,\n\u001b[1;32m    846\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    847\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    848\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    849\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m    850\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    851\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    852\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    853\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    854\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    855\u001b[0m )\n\u001b[1;32m    856\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    857\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:529\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    520\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    521\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    522\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    526\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    527\u001b[0m     )\n\u001b[1;32m    528\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 529\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    530\u001b[0m         hidden_states,\n\u001b[1;32m    531\u001b[0m         attention_mask,\n\u001b[1;32m    532\u001b[0m         layer_head_mask,\n\u001b[1;32m    533\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    534\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    535\u001b[0m         past_key_value,\n\u001b[1;32m    536\u001b[0m         output_attentions,\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    539\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    540\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:413\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    402\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    403\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    410\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    411\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    414\u001b[0m         hidden_states,\n\u001b[1;32m    415\u001b[0m         attention_mask,\n\u001b[1;32m    416\u001b[0m         head_mask,\n\u001b[1;32m    417\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    418\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    419\u001b[0m     )\n\u001b[1;32m    420\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    422\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:340\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    331\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    332\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    338\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    339\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 340\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    341\u001b[0m         hidden_states,\n\u001b[1;32m    342\u001b[0m         attention_mask,\n\u001b[1;32m    343\u001b[0m         head_mask,\n\u001b[1;32m    344\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    345\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    346\u001b[0m         past_key_value,\n\u001b[1;32m    347\u001b[0m         output_attentions,\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    350\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/transformers/models/roberta/modeling_roberta.py:219\u001b[0m, in \u001b[0;36mRobertaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    217\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 219\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    220\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    222\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/jiang2023low/lib/python3.8/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "D = torch.empty(size = [0])\n",
    "NCD = torch.empty(size = [0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, (label, text, offsets, classes, ncd) in tqdm(enumerate(test_loader)):\n",
    "        btsz = len(offsets) // 2\n",
    "        h = model(label)\n",
    "        delta = F.pairwise_distance(h[:btsz], h[btsz:]) / 10\n",
    "        D = torch.cat((D, delta.cpu()))\n",
    "        NCD = torch.cat((NCD, ncd.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.3380, 0.3194, 0.3309, 0.3197, 0.3222, 0.3132, 0.3106, 0.3014],\n",
       "        device='cuda:0'),\n",
       " tensor([0.3371, 0.3243, 0.3650, 0.2964, 0.3482, 0.3010, 0.3021, 0.2563],\n",
       "        device='cuda:0'))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta, ncd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8399, device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson(delta, ncd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(D, NCD);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in roberta_model.parameters():\n",
    "#     print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in roberta_model.parameters():\n",
    "#     print (i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
    "\n",
    "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Pass the inputs through the RoBERTa model\n",
    "outputs = model(**inputs)\n",
    "\n",
    "sentence_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# You can also take the average of all the token embeddings to get sentence embeddings\n",
    "# sentence_embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "sentences = [\"This is a sample sentence.\", \"Another sentence goes here.\", \"And one more sentence.\"]\n",
    "\n",
    "# Get the sentence embeddings for the batch of sentences\n",
    "sentence_embeddings = get_sentence_embeddings(sentences)\n",
    "\n",
    "print(sentence_embeddings.shape)  # Should print: torch.Size([3, 768]) for 3 sentences in the batch with RoBERTa-base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the RoBERTa model and get the embeddings for each sentence in each batch\n",
    "#for batch in train_loader:  # You can change this to val_loader or test_loader if needed\n",
    "#    raw_list, _, _, _ = batch  # Use only the raw texts (list of sentences)\n",
    "#\n",
    "#    # Get RoBERTa embeddings for the sentences in the batch\n",
    "#    embeddings = get_roberta_embeddings(raw_list)\n",
    "#    \n",
    "#\n",
    "#\n",
    "#\n",
    "#    # Perform your downstream task using the embeddings\n",
    "#    # For example, if you have a classification head, you can do the following:\n",
    "#    #logits = classification_head(embeddings)\n",
    "#\n",
    "#    # Detach logits from the computation graph to avoid backpropagating through them\n",
    "#    #logits_detach = logits.detach()\n",
    "#    target_labels = torch.randn(embeddings.shape).to(device)  # Replace with your actual target labels\n",
    "#    \n",
    "#    # Compute the loss (you need to define your own loss function based on your task)\n",
    "#    # For example, if it's a classification task, you can use CrossEntropyLoss:\n",
    "#    loss = criterion(embeddings, target_labels)  # Replace target_labels with your actual target labels\n",
    "#\n",
    "#    # Perform backward pass and update the weights\n",
    "#    optimizer.zero_grad()\n",
    "#    loss.backward()\n",
    "#    optimizer.step()\n",
    "#    break\n",
    "#\n",
    "#    # Now 'embeddings' contains the embeddings of each sentence in the batch.\n",
    "#    # The shape of 'embeddings' will be (num_sentences_in_batch, embedding_size).\n",
    "#    # You can further process these embeddings or use them for your fine-tuning task.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
